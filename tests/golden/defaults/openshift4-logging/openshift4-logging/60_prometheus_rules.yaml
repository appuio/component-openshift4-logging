apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  annotations: {}
  labels:
    name: syn-logging-rules
  name: syn-logging-rules
  namespace: openshift-logging
spec:
  groups:
    - name: logging_fluentd.alerts
      rules:
        - alert: SYN_FluentdNodeDown
          annotations:
            message: Prometheus could not scrape fluentd {{ $labels.instance }} for
              more than 10m.
            summary: Fluentd cannot be scraped
          expr: 'up{job="collector"} == 0 or absent(up{job="collector"}) == 1

            '
          for: 10m
          labels:
            service: fluentd
            severity: critical
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_FluentdQueueLengthIncreasing
          annotations:
            message: For the last hour, fluentd {{ $labels.instance }} output '{{
              $labels.plugin_id }}' average buffer queue length has increased continuously.
            summary: Fluentd is unable to keep up with traffic over time for forwarder
              output {{ $labels.plugin_id }}.
          expr: '( 0 * (deriv(fluentd_output_status_emit_records[1m] offset 1h)))  +
            on(pod,plugin_id)  ( deriv(fluentd_output_status_buffer_queue_length[10m])
            > 0 and delta(fluentd_output_status_buffer_queue_length[1h]) > 1 )

            '
          for: 12h
          labels:
            service: fluentd
            severity: Warning
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_FluentDHighErrorRate
          annotations:
            message: '{{ $value }}% of records have resulted in an error by fluentd
              {{ $labels.instance }}.'
            summary: FluentD output errors are high
          expr: "100 * (\n  sum by(instance)(rate(fluentd_output_status_num_errors[2m]))\n\
            /\n  sum by(instance)(rate(fluentd_output_status_emit_records[2m]))\n\
            ) > 10\n"
          for: 15m
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_FluentDVeryHighErrorRate
          annotations:
            message: '{{ $value }}% of records have resulted in an error by fluentd
              {{ $labels.instance }}.'
            summary: FluentD output errors are very high
          expr: "100 * (\n  sum by(instance)(rate(fluentd_output_status_num_errors[2m]))\n\
            /\n  sum by(instance)(rate(fluentd_output_status_emit_records[2m]))\n\
            ) > 25\n"
          for: 15m
          labels:
            severity: critical
            syn: 'true'
            syn_component: openshift4-logging
    - name: elasticsearch_node_storage.alerts
      rules:
        - alert: SYN_ElasticsearchExpectNodeToReachDiskWatermark
          annotations:
            message: Expecting to reach disk low watermark at {{ $labels.node }} node
              in {{ $labels.cluster }} cluster in 72 hours. When reaching the watermark
              no new shards will be allocated to this node anymore. You should consider
              adding more disk to the node.
            runbook_url: https://hub.syn.tools/openshift4-logging/runbooks/SYN_ElasticsearchExpectNodeToReachDiskWatermark.html
            summary: Expecting to Reach Disk Low Watermark in 72 Hours
          expr: "sum by(cluster, instance, node) (\n  (1 - (predict_linear(es_fs_path_available_bytes[72h],\
            \ 259200) / es_fs_path_total_bytes)) * 100\n) > 85\n"
          for: 6h
          labels:
            severity: warning
