apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  annotations: {}
  labels:
    name: syn-elasticsearch-logging-rules
  name: syn-elasticsearch-logging-rules
  namespace: openshift-logging
spec:
  groups:
    - name: logging_elasticsearch.alerts
      rules:
        - alert: SYN_ElasticsearchClusterNotHealthy
          annotations:
            message: Cluster {{ $labels.cluster }} health status has been RED for
              at least 7m. Cluster does not accept writes, shards may be missing or
              master node hasn't been elected yet.
            runbook_url: https://github.com/openshift/elasticsearch-operator/blob/master/docs/alerts.md#Elasticsearch-Cluster-Health-is-Red
            summary: Cluster health status is RED
          expr: |
            sum by (cluster) (es_cluster_status == 2)
          for: 7m
          labels:
            namespace: openshift-logging
            severity: critical
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_ElasticsearchClusterNotHealthy
          annotations:
            message: Cluster {{ $labels.cluster }} health status has been YELLOW for
              at least 20m. Some shard replicas are not allocated.
            runbook_url: https://github.com/openshift/elasticsearch-operator/blob/master/docs/alerts.md#Elasticsearch-Cluster-Health-is-Yellow
            summary: Cluster health status is YELLOW
          expr: |
            sum by (cluster) (es_cluster_status == 1)
          for: 20m
          labels:
            namespace: openshift-logging
            severity: warning
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_ElasticsearchWriteRequestsRejectionJumps
          annotations:
            message: High Write Rejection Ratio at {{ $labels.node }} node in {{ $labels.cluster
              }} cluster. This node may not be keeping up with the indexing speed.
            runbook_url: https://github.com/openshift/elasticsearch-operator/blob/master/docs/alerts.md#Elasticsearch-Write-Requests-Rejection-Jumps
            summary: High Write Rejection Ratio - {{ $value }}%
          expr: |
            round( writing:reject_ratio:rate2m * 100, 0.001 ) > 5
          for: 10m
          labels:
            namespace: openshift-logging
            severity: warning
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_ElasticsearchNodeDiskWatermarkReached
          annotations:
            message: Disk High Watermark Reached at {{ $labels.pod }} pod. Some shards
              will be re-allocated to different nodes if possible. Make sure more
              disk space is added to the node or drop old indices allocated to this
              node.
            runbook_url: https://github.com/openshift/elasticsearch-operator/blob/master/docs/alerts.md#Elasticsearch-Node-Disk-High-Watermark-Reached
            summary: Disk High Watermark Reached - disk saturation is {{ $value }}%
          expr: |
            sum by (instance, pod) (
              round(
                (1 - (
                  es_fs_path_available_bytes /
                  es_fs_path_total_bytes
                )
              ) * 100, 0.001)
            ) > on(instance, pod) es_cluster_routing_allocation_disk_watermark_high_pct
          for: 5m
          labels:
            namespace: openshift-logging
            severity: critical
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_ElasticsearchNodeDiskWatermarkReached
          annotations:
            message: Disk Flood Stage Watermark Reached at {{ $labels.pod }}. Every
              index having a shard allocated on this node is enforced a read-only
              block. The index block must be released manually when the disk utilization
              falls below the high watermark.
            runbook_url: https://github.com/openshift/elasticsearch-operator/blob/master/docs/alerts.md#Elasticsearch-Node-Disk-Flood-Watermark-Reached
            summary: Disk Flood Stage Watermark Reached - disk saturation is {{ $value
              }}%
          expr: |
            sum by (instance, pod) (
              round(
                (1 - (
                  es_fs_path_available_bytes /
                  es_fs_path_total_bytes
                )
              ) * 100, 0.001)
            ) > on(instance, pod) es_cluster_routing_allocation_disk_watermark_flood_stage_pct
          for: 5m
          labels:
            namespace: openshift-logging
            severity: critical
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_ElasticsearchDiskSpaceRunningLow
          annotations:
            message: Cluster {{ $labels.cluster }} is predicted to be out of disk
              space within the next 6h.
            runbook_url: https://github.com/openshift/elasticsearch-operator/blob/master/docs/alerts.md#Elasticsearch-Disk-Space-is-Running-Low
            summary: Cluster low on disk space
          expr: |
            sum(predict_linear(es_fs_path_available_bytes[6h], 6 * 3600)) < 0
          for: 1h
          labels:
            namespace: openshift-logging
            severity: critical
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_ElasticsearchNodeDiskWatermarkReached
          annotations:
            message: Disk Low Watermark is predicted to be reached within the next
              6h at {{ $labels.pod }} pod. Shards can not be allocated to this node
              anymore. You should consider adding more disk to the node.
            runbook_url: https://github.com/openshift/elasticsearch-operator/blob/master/docs/alerts.md#Elasticsearch-Node-Disk-Low-Watermark-Reached
            summary: Disk Low Watermark is predicted to be reached within next 6h.
          expr: |
            sum by (instance, pod) (
              round(
                (1 - (
                  predict_linear(es_fs_path_available_bytes[3h], 6 * 3600) /
                  predict_linear(es_fs_path_total_bytes[3h], 6 * 3600)
                )
              ) * 100, 0.001)
            ) > on(instance, pod) es_cluster_routing_allocation_disk_watermark_low_pct
          for: 1h
          labels:
            namespace: openshift-logging
            severity: warning
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_ElasticsearchNodeDiskWatermarkReached
          annotations:
            message: Disk High Watermark is predicted to be reached within the next
              6h at {{ $labels.pod }} pod. Some shards will be re-allocated to different
              nodes if possible. Make sure more disk space is added to the node or
              drop old indices allocated to this node.
            runbook_url: https://github.com/openshift/elasticsearch-operator/blob/master/docs/alerts.md#Elasticsearch-Node-Disk-High-Watermark-Reached
            summary: Disk High Watermark is predicted to be reached within next 6h.
          expr: |
            sum by (instance, pod) (
              round(
                (1 - (
                  predict_linear(es_fs_path_available_bytes[3h], 6 * 3600) /
                  predict_linear(es_fs_path_total_bytes[3h], 6 * 3600)
                )
              ) * 100, 0.001)
            ) > on(instance, pod) es_cluster_routing_allocation_disk_watermark_high_pct
          for: 1h
          labels:
            namespace: openshift-logging
            severity: warning
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_ElasticsearchNodeDiskWatermarkReached
          annotations:
            message: Disk Flood Stage Watermark is predicted to be reached within
              the next 6h at {{ $labels.pod }}. Every index having a shard allocated
              on this node is enforced a read-only block. The index block must be
              released manually when the disk utilization falls below the high watermark.
            runbook_url: https://github.com/openshift/elasticsearch-operator/blob/master/docs/alerts.md#Elasticsearch-Node-Disk-Flood-Watermark-Reached
            summary: Disk Flood Stage Watermark is predicted to be reached within
              next 6h.
          expr: |
            sum by (instance, pod) (
              round(
                (1 - (
                  predict_linear(es_fs_path_available_bytes[3h], 6 * 3600) /
                  predict_linear(es_fs_path_total_bytes[3h], 6 * 3600)
                )
              ) * 100, 0.001)
            ) > on(instance, pod) es_cluster_routing_allocation_disk_watermark_flood_stage_pct
          for: 1h
          labels:
            namespace: openshift-logging
            severity: warning
            syn: 'true'
            syn_component: openshift4-logging
    - name: logging_fluentd.alerts
      rules:
        - alert: SYN_FluentdNodeDown
          annotations:
            message: Prometheus could not scrape fluentd {{ $labels.container }} for
              more than 10m.
            summary: Fluentd cannot be scraped
          expr: |
            up{job = "collector", container = "collector"} == 0 or absent(up{job="collector", container="collector"}) == 1
          for: 10m
          labels:
            namespace: openshift-logging
            service: collector
            severity: critical
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_FluentDHighErrorRate
          annotations:
            message: '{{ $value }}% of records have resulted in an error by fluentd
              {{ $labels.instance }}.'
            summary: FluentD output errors are high
          expr: |
            100 * (
              sum by(instance)(rate(fluentd_output_status_num_errors[2m]))
            /
              sum by(instance)(rate(fluentd_output_status_emit_records[2m]))
            ) > 10
          for: 15m
          labels:
            namespace: openshift-logging
            severity: warning
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_FluentDVeryHighErrorRate
          annotations:
            message: '{{ $value }}% of records have resulted in an error by fluentd
              {{ $labels.instance }}.'
            summary: FluentD output errors are very high
          expr: |
            100 * (
              sum by(instance)(rate(fluentd_output_status_num_errors[2m]))
            /
              sum by(instance)(rate(fluentd_output_status_emit_records[2m]))
            ) > 25
          for: 15m
          labels:
            namespace: openshift-logging
            severity: critical
            syn: 'true'
            syn_component: openshift4-logging
    - name: elasticsearch_node_storage.alerts
      rules:
        - alert: SYN_ElasticsearchExpectNodeToReachDiskWatermark
          annotations:
            message: Expecting to reach disk low watermark at {{ $labels.node }} node
              in {{ $labels.cluster }} cluster in 72 hours. When reaching the watermark
              no new shards will be allocated to this node anymore. You should consider
              adding more disk to the node.
            runbook_url: https://hub.syn.tools/openshift4-logging/runbooks/SYN_ElasticsearchExpectNodeToReachDiskWatermark.html
            summary: Expecting to Reach Disk Low Watermark in 72 Hours
          expr: |
            sum by(cluster, instance, node) (
              (1 - (predict_linear(es_fs_path_available_bytes[72h], 259200) / es_fs_path_total_bytes)) * 100
            ) > 85
          for: 6h
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-logging
