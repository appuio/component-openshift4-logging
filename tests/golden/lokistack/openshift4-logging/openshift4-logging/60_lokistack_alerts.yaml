apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  annotations: {}
  labels:
    name: syn-loki-logging-rules
  name: syn-loki-logging-rules
  namespace: openshift-logging
spec:
  groups:
    - name: logging_loki.alerts
      rules:
        - alert: SYN_LokiRequestErrors
          annotations:
            message: '{{ $labels.job }} {{ $labels.route }} is experiencing {{ printf
              "%.2f" $value }}% errors.'
            runbook_url: https://github.com/grafana/loki/blob/main/operator/docs/lokistack/sop.md#Loki-Request-Errors
            summary: At least 10% of requests are responded by 5xx server errors.
          expr: "sum(\n  job_namespace_route_statuscode:loki_request_duration_seconds_count:irate1m{status_code=~\"\
            5..\"}\n) by (job, namespace, route)\n/\nsum(\n  job_namespace_route_statuscode:loki_request_duration_seconds_count:irate1m\n\
            ) by (job, namespace, route)\n* 100\n> 10\n"
          for: 15m
          labels:
            severity: critical
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_LokiStackWriteRequestErrors
          annotations:
            message: '{{ printf "%.2f" $value }}% of write requests from {{ $labels.job
              }} in {{ $labels.namespace }} are returned with server errors.'
            runbook_url: https://github.com/grafana/loki/blob/main/operator/docs/lokistack/sop.md#LokiStack-Write-Request-Errors
            summary: At least 10% of write requests to the lokistack-gateway are responded
              with 5xx server errors.
          expr: "sum(\n  code_handler_job_namespace:lokistack_gateway_http_requests:irate1m{code=~\"\
            5..\", handler=\"push\"}\n) by (job, namespace)\n/\nsum(\n  code_handler_job_namespace:lokistack_gateway_http_requests:irate1m{handler=\"\
            push\"}\n) by (job, namespace)\n* 100\n> 10\n"
          for: 15m
          labels:
            severity: critical
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_LokiStackReadRequestErrors
          annotations:
            message: '{{ printf "%.2f" $value }}% of query requests from {{ $labels.job
              }} in {{ $labels.namespace }} are returned with server errors.'
            runbook_url: https://github.com/grafana/loki/blob/main/operator/docs/lokistack/sop.md#LokiStack-Read-Request-Errors
            summary: At least 10% of query requests to the lokistack-gateway are responded
              with 5xx server errors.
          expr: "sum(\n  code_handler_job_namespace:lokistack_gateway_http_requests:irate1m{code=~\"\
            5..\", handler=~\"query|query_range|label|labels|label_values\"}\n) by\
            \ (job, namespace)\n/\nsum(\n  code_handler_job_namespace:lokistack_gateway_http_requests:irate1m{handler=~\"\
            query|query_range|label|labels|label_values\"}\n) by (job, namespace)\n\
            * 100\n> 10\n"
          for: 15m
          labels:
            severity: critical
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_LokiRequestPanics
          annotations:
            message: '{{ $labels.job }} is experiencing an increase of {{ $value }}
              panics.'
            runbook_url: https://github.com/grafana/loki/blob/main/operator/docs/lokistack/sop.md#Loki-Request-Panics
            summary: A panic was triggered.
          expr: "sum(\n  increase(\n    loki_panic_total[10m]\n  )\n) by (job, namespace)\n\
            > 0\n"
          labels:
            severity: critical
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_LokiRequestLatency
          annotations:
            message: '{{ $labels.job }} {{ $labels.route }} is experiencing {{ printf
              "%.2f" $value }}s 99th percentile latency.'
            runbook_url: https://github.com/grafana/loki/blob/main/operator/docs/lokistack/sop.md#Loki-Request-Latency
            summary: The 99th percentile is experiencing high latency (higher than
              1 second).
          expr: "histogram_quantile(0.99,\n  sum(\n    irate(\n      loki_request_duration_seconds_bucket{route!~\"\
            (?i).*tail.*\"}[1m]\n    )\n  ) by (job, le, namespace, route)\n)\n> 1\n"
          for: 15m
          labels:
            severity: critical
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_LokiTenantRateLimit
          annotations:
            message: '{{ $labels.job }} {{ $labels.route }} is experiencing 429 errors.'
            runbook_url: https://github.com/grafana/loki/blob/main/operator/docs/lokistack/sop.md#Loki-Tenant-Rate-Limit
            summary: At least 10% of requests are responded with the rate limit error
              code.
          expr: "sum(\n  job_namespace_route_statuscode:loki_request_duration_seconds_count:irate1m{status_code=\"\
            429\"}\n) by (job, namespace, route)\n/\nsum(\n  job_namespace_route_statuscode:loki_request_duration_seconds_count:irate1m\n\
            ) by (job, namespace, route)\n* 100\n> 10\n"
          for: 15m
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_LokiStorageSlowWrite
          annotations:
            message: The storage path is experiencing slow write response rates.
            runbook_url: https://github.com/grafana/loki/blob/main/operator/docs/lokistack/sop.md#Loki-Storage-Slow-Write
            summary: The storage path is experiencing slow write response rates.
          expr: "histogram_quantile(0.99,\n  sum(\n    job_le_namespace_operation:loki_boltdb_shipper_request_duration_seconds_bucket:rate5m{operation=\"\
            WRITE\"}\n  ) by (job, le, namespace)\n)\n> 1\n"
          for: 15m
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_LokiStorageSlowRead
          annotations:
            message: The storage path is experiencing slow read response rates.
            runbook_url: https://github.com/grafana/loki/blob/main/operator/docs/lokistack/sop.md#Loki-Storage-Slow-Read
            summary: The storage path is experiencing slow read response rates.
          expr: "histogram_quantile(0.99,\n  sum(\n    job_le_namespace_operation:loki_boltdb_shipper_request_duration_seconds_bucket:rate5m{operation=\"\
            Shipper.Query\"}\n  ) by (job, le, namespace)\n)\n> 5\n"
          for: 15m
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_LokiWritePathHighLoad
          annotations:
            message: The write path is experiencing high load.
            runbook_url: https://github.com/grafana/loki/blob/main/operator/docs/lokistack/sop.md#Loki-Write-Path-High-Load
            summary: The write path is experiencing high load, causing backpressure
              storage flushing.
          expr: "sum(\n  loki_ingester_wal_replay_flushing\n) by (job, namespace)\n\
            > 0\n"
          for: 15m
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-logging
        - alert: SYN_LokiReadPathHighLoad
          annotations:
            message: The read path is experiencing high load.
            runbook_url: https://github.com/grafana/loki/blob/main/operator/docs/lokistack/sop.md#Loki-Read-Path-High-Load
            summary: The read path has high volume of queries, causing longer response
              times.
          expr: "histogram_quantile(0.99,\n  sum(\n    rate(\n      loki_logql_querystats_latency_seconds_bucket[5m]\n\
            \    )\n  ) by (job, le, namespace)\n)\n> 30\n"
          for: 15m
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-logging
