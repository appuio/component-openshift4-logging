apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: collector
  namespace: openshift-logging
spec:
  groups:
  - name: logging_collector.alerts
    rules:
    - alert: CollectorNodeDown
      annotations:
        message: "Prometheus could not scrape {{ $labels.namespace }}/{{ $labels.pod }} collector component for more than 10m."
        summary: "Collector cannot be scraped"
      expr: |
        up{app_kubernetes_io_component = "collector", app_kubernetes_io_part_of = "cluster-logging"} == 0
      for: 10m
      labels:
        service: collector
        severity: critical
    - alert: CollectorHighErrorRate
      annotations:
        message: "{{ $value }}% of records have resulted in an error by {{ $labels.namespace }}/{{ $labels.pod }} collector component."
        summary: "{{ $labels.namespace }}/{{ $labels.pod }} collector component errors are high"
      expr: |
        100 * (
            collector:log_num_errors:sum_rate{app_kubernetes_io_part_of = "cluster-logging"}
          /
            collector:received_events:sum_rate{app_kubernetes_io_part_of = "cluster-logging"}
          ) > 0.001
      for: 15m
      labels:
        service: collector
        severity: critical
    - alert: CollectorVeryHighErrorRate
      annotations:
        message: "{{ $value }}% of records have resulted in an error by {{ $labels.namespace }}/{{ $labels.pod }} collector component."
        summary: "{{ $labels.namespace }}/{{ $labels.pod }} collector component errors are very high"
      expr: |
        100 * (
            collector:log_num_errors:sum_rate{app_kubernetes_io_part_of = "cluster-logging"}
          /
            collector:received_events:sum_rate{app_kubernetes_io_part_of = "cluster-logging"}
          ) > 0.05
      for: 15m
      labels:
        service: collector
        severity: critical
    - alert: FluentdQueueLengthIncreasing
      annotations:
        message: "For the last hour, fluentd {{ $labels.pod }} output '{{ $labels.plugin_id }}' average buffer queue length has increased continuously."
        summary: "Fluentd pod {{ $labels.pod }} is unable to keep up with traffic over time for forwarder output {{ $labels.plugin_id }}."
      expr: |
        sum by (pod,plugin_id) ( 0 * (deriv(fluentd_output_status_emit_records[1m] offset 1h)))  + on(pod,plugin_id)  ( deriv(fluentd_output_status_buffer_queue_length[10m]) > 0 and delta(fluentd_output_status_buffer_queue_length[1h]) > 1 )
      for: 1h
      labels:
        service: collector
        severity: Warning
  - name: logging_clusterlogging_telemetry.rules
    rules:
    - expr: |
        sum by(cluster)(log_collected_bytes_total)
      record: cluster:log_collected_bytes_total:sum
    - expr: |
        sum by(cluster)(log_logged_bytes_total)
      record: cluster:log_logged_bytes_total:sum
    - expr: |
        sum by(pod, namespace, app_kubernetes_io_part_of)(rate(vector_component_errors_total[2m])) or sum by(pod, namespace, app_kubernetes_io_part_of)(rate(fluentd_output_status_num_errors[2m]))
      record: collector:log_num_errors:sum_rate
    - expr: |
        sum by(pod, namespace, app_kubernetes_io_part_of)(rate(vector_component_received_events_total[2m])) or sum by(pod, namespace, app_kubernetes_io_part_of)(rate(fluentd_output_status_emit_records[2m]))
      record: collector:received_events:sum_rate
